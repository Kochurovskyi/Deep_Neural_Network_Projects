{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Machine learning vs. Deep Neural Network (regression)\n",
    "## Open required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.impute import MissingIndicator\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.metrics import MAPE\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, BatchNormalization, Activation, add, Dropout\n",
    "from tensorflow.keras import regularizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open and read data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"test_no_target.csv\", index_col=0)\n",
    "zipcodes_df = pd.read_csv(\"zipcodes.csv\", index_col=0)\n",
    "\n",
    "train_df = pd.merge(train_df.reset_index(), zipcodes_df.drop_duplicates(\"zipcode\"), on=\"zipcode\", how=\"left\")\n",
    "test_df = pd.merge(test_df.reset_index(), zipcodes_df.drop_duplicates(\"zipcode\"), on=\"zipcode\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>engine_capacity</th>\n",
       "      <th>type</th>\n",
       "      <th>registration_year</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>power</th>\n",
       "      <th>model</th>\n",
       "      <th>mileage</th>\n",
       "      <th>fuel</th>\n",
       "      <th>brand</th>\n",
       "      <th>damage</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>insurance_price</th>\n",
       "      <th>price</th>\n",
       "      <th>city</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48298</td>\n",
       "      <td>2.0</td>\n",
       "      <td>bus</td>\n",
       "      <td>2006</td>\n",
       "      <td>auto</td>\n",
       "      <td>140</td>\n",
       "      <td>c4</td>\n",
       "      <td>150000</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>citroen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49191</td>\n",
       "      <td>380.0</td>\n",
       "      <td>4267</td>\n",
       "      <td>Belm</td>\n",
       "      <td>52.30476</td>\n",
       "      <td>8.12846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>vito</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mercedes_benz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2457</td>\n",
       "      <td>Gelsenkirchen</td>\n",
       "      <td>51.51750</td>\n",
       "      <td>7.08575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92754</td>\n",
       "      <td>2.2</td>\n",
       "      <td>limousine</td>\n",
       "      <td>2010</td>\n",
       "      <td>manual</td>\n",
       "      <td>175</td>\n",
       "      <td>mondeo</td>\n",
       "      <td>125000</td>\n",
       "      <td>diesel</td>\n",
       "      <td>ford</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59229</td>\n",
       "      <td>930.0</td>\n",
       "      <td>10374</td>\n",
       "      <td>Ahlen, Westfalen</td>\n",
       "      <td>51.75972</td>\n",
       "      <td>7.89694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>auto</td>\n",
       "      <td>265</td>\n",
       "      <td>andere</td>\n",
       "      <td>150000</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>ford</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39365</td>\n",
       "      <td>680.0</td>\n",
       "      <td>7098</td>\n",
       "      <td>Druxberge</td>\n",
       "      <td>52.15648</td>\n",
       "      <td>11.30968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76981</td>\n",
       "      <td>NaN</td>\n",
       "      <td>convertible</td>\n",
       "      <td>3</td>\n",
       "      <td>manual</td>\n",
       "      <td>109</td>\n",
       "      <td>2_reihe</td>\n",
       "      <td>150000</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>peugeot</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2365</td>\n",
       "      <td>Stadecken-Elsheim</td>\n",
       "      <td>49.91220</td>\n",
       "      <td>8.12528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>50429</td>\n",
       "      <td>1.4</td>\n",
       "      <td>limousine</td>\n",
       "      <td>2006</td>\n",
       "      <td>manual</td>\n",
       "      <td>75</td>\n",
       "      <td>golf</td>\n",
       "      <td>90000</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35745</td>\n",
       "      <td>500.0</td>\n",
       "      <td>4686</td>\n",
       "      <td>Herborn, Hessen</td>\n",
       "      <td>50.68330</td>\n",
       "      <td>8.31667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>64425</td>\n",
       "      <td>1.3</td>\n",
       "      <td>small car</td>\n",
       "      <td>4</td>\n",
       "      <td>manual</td>\n",
       "      <td>60</td>\n",
       "      <td>fiesta</td>\n",
       "      <td>150000</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>ford</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864</td>\n",
       "      <td>Frankfurt am Main Fechenheim</td>\n",
       "      <td>50.11670</td>\n",
       "      <td>8.68333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>90761</td>\n",
       "      <td>NaN</td>\n",
       "      <td>limousine</td>\n",
       "      <td>1996</td>\n",
       "      <td>manual</td>\n",
       "      <td>150</td>\n",
       "      <td>5er</td>\n",
       "      <td>150000</td>\n",
       "      <td>gasoline</td>\n",
       "      <td>bmw</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28309</td>\n",
       "      <td>130.0</td>\n",
       "      <td>2275</td>\n",
       "      <td>Bremen</td>\n",
       "      <td>53.07516</td>\n",
       "      <td>8.80777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>39709</td>\n",
       "      <td>NaN</td>\n",
       "      <td>limousine</td>\n",
       "      <td>2007</td>\n",
       "      <td>manual</td>\n",
       "      <td>122</td>\n",
       "      <td>1er</td>\n",
       "      <td>100000</td>\n",
       "      <td>diesel</td>\n",
       "      <td>bmw</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83623</td>\n",
       "      <td>500.0</td>\n",
       "      <td>8144</td>\n",
       "      <td>Dietramszell</td>\n",
       "      <td>47.85000</td>\n",
       "      <td>11.60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>25524</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1996</td>\n",
       "      <td>manual</td>\n",
       "      <td>0</td>\n",
       "      <td>golf</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26789</td>\n",
       "      <td>220.0</td>\n",
       "      <td>1592</td>\n",
       "      <td>Leer (Ostfriesland)</td>\n",
       "      <td>53.23765</td>\n",
       "      <td>7.46720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  engine_capacity         type  registration_year gearbox  power  \\\n",
       "0      48298              2.0          bus               2006    auto    140   \n",
       "1      81047              NaN          NaN               2016     NaN      0   \n",
       "2      92754              2.2    limousine               2010  manual    175   \n",
       "3      46007              NaN          NaN               2000    auto    265   \n",
       "4      76981              NaN  convertible                  3  manual    109   \n",
       "...      ...              ...          ...                ...     ...    ...   \n",
       "49995  50429              1.4    limousine               2006  manual     75   \n",
       "49996  64425              1.3    small car                  4  manual     60   \n",
       "49997  90761              NaN    limousine               1996  manual    150   \n",
       "49998  39709              NaN    limousine               2007  manual    122   \n",
       "49999  25524              NaN          NaN               1996  manual      0   \n",
       "\n",
       "         model  mileage      fuel          brand  damage  zipcode  \\\n",
       "0           c4   150000  gasoline        citroen     0.0    49191   \n",
       "1         vito   150000       NaN  mercedes_benz     NaN    45896   \n",
       "2       mondeo   125000    diesel           ford     0.0    59229   \n",
       "3       andere   150000  gasoline           ford     0.0    39365   \n",
       "4      2_reihe   150000  gasoline        peugeot     0.0    55271   \n",
       "...        ...      ...       ...            ...     ...      ...   \n",
       "49995     golf    90000  gasoline     volkswagen     0.0    35745   \n",
       "49996   fiesta   150000  gasoline           ford     0.0    60386   \n",
       "49997      5er   150000  gasoline            bmw     0.0    28309   \n",
       "49998      1er   100000    diesel            bmw     0.0    83623   \n",
       "49999     golf   150000       NaN     volkswagen     0.0    26789   \n",
       "\n",
       "       insurance_price  price                          city  latitude  \\\n",
       "0                380.0   4267                          Belm  52.30476   \n",
       "1                  NaN   2457                 Gelsenkirchen  51.51750   \n",
       "2                930.0  10374              Ahlen, Westfalen  51.75972   \n",
       "3                680.0   7098                     Druxberge  52.15648   \n",
       "4                  NaN   2365             Stadecken-Elsheim  49.91220   \n",
       "...                ...    ...                           ...       ...   \n",
       "49995            500.0   4686               Herborn, Hessen  50.68330   \n",
       "49996              NaN    864  Frankfurt am Main Fechenheim  50.11670   \n",
       "49997            130.0   2275                        Bremen  53.07516   \n",
       "49998            500.0   8144                  Dietramszell  47.85000   \n",
       "49999            220.0   1592           Leer (Ostfriesland)  53.23765   \n",
       "\n",
       "       longitude  \n",
       "0        8.12846  \n",
       "1        7.08575  \n",
       "2        7.89694  \n",
       "3       11.30968  \n",
       "4        8.12528  \n",
       "...          ...  \n",
       "49995    8.31667  \n",
       "49996    8.68333  \n",
       "49997    8.80777  \n",
       "49998   11.60000  \n",
       "49999    7.46720  \n",
       "\n",
       "[50000 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\"type\", \"gearbox\", \"model\", \"fuel\", \"brand\", \"city\"]\n",
    "cont_missing_features = [\"engine_capacity\", \"damage\", \"insurance_price\", \"latitude\", \"longitude\"]\n",
    "cat_missing_features = [\"type\", \"gearbox\", \"model\", \"fuel\", \"city\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for dataframe manipulations\n",
    "* mape - calculated mean absolute percentage error\n",
    "* concatenate dataframes\n",
    "* split dataframes\n",
    "* create submit-file for test set\n",
    "* common preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def zip_dataframes(*dataframes):\n",
    "    for idx, dataframe in enumerate(dataframes):\n",
    "        dataframe[\"df_order\"] = idx\n",
    "    return pd.concat(dataframes)\n",
    "\n",
    "def unzip_dataframes(dataframe):\n",
    "    dataframes = []\n",
    "    for n in dataframe[\"df_order\"].unique().tolist():\n",
    "        dataframes.append(dataframe[dataframe[\"df_order\"] == n].drop(columns=\"df_order\"))\n",
    "    return dataframes\n",
    "    \n",
    "\n",
    "def create_submit_df(test_df, preds):\n",
    "    submit_df = pd.DataFrame({\n",
    "        \"Id\": test_df[\"index\"],\n",
    "        \"Predicted\": preds,\n",
    "    })\n",
    "    return submit_df\n",
    "\n",
    "def preprocessing(train_df, test_df, funcs):\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    for func in funcs:\n",
    "        train_df, test_df = func(train_df, test_df)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing functions \n",
    "* fill NaNs with zeros\n",
    "* fill NaNs as per strategy\n",
    "* manual preprocessing\n",
    "* drop useless columns\n",
    "* drop outlayers\n",
    "* cat. features encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nan_with_zero(train_df, test_df):\n",
    "    for cat_feature in cat_features:\n",
    "        train_df[cat_feature] = train_df[cat_feature].fillna(\"nan\")\n",
    "        test_df[cat_feature] = test_df[cat_feature].fillna(\"nan\")\n",
    "    train_df = train_df.fillna(0)\n",
    "    test_df = test_df.fillna(0)\n",
    "    return train_df, test_df\n",
    "\n",
    "def impute_nan(train_df, test_df):\n",
    "    for cont_missing_feature in cont_missing_features:\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "        imp.fit(pd.concat([train_df, test_df])[[cont_missing_feature]])\n",
    "        train_df[cont_missing_feature] = imp.transform(train_df[[cont_missing_feature]])\n",
    "        test_df[cont_missing_feature] = imp.transform(test_df[[cont_missing_feature]])\n",
    "\n",
    "    for cat_missing_feature in cat_missing_features:\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=\"nan\")\n",
    "\n",
    "        imp.fit(pd.concat([train_df, test_df])[[cat_missing_feature]])\n",
    "        train_df[cat_missing_feature] = imp.transform(train_df[[cat_missing_feature]])\n",
    "        test_df[cat_missing_feature] = imp.transform(test_df[[cat_missing_feature]])\n",
    "    return train_df, test_df\n",
    "\n",
    "def drop_columns(train_df, test_df):\n",
    "    drop_columns = [\"index\"]\n",
    "    train_df = train_df.drop(columns=drop_columns)\n",
    "    test_df = test_df.drop(columns=drop_columns)\n",
    "    return train_df, test_df\n",
    "\n",
    "def drop_price_outliers(train_df, test_df):\n",
    "    upper_bound = np.quantile(train_df.price, 0.95)\n",
    "    train_df = train_df[train_df.price <= upper_bound]\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def drop_insurance_price_outliers(train_df, test_df):\n",
    "    upper_bound = np.quantile(train_df.insurance_price, 0.99)\n",
    "    train_df = train_df[train_df.insurance_price <= upper_bound]\n",
    "    return train_df, test_df\n",
    "\n",
    "def fill_insurance_price(train_df, test_df):\n",
    "    train_df.loc[train_df.insurance_price.isna(), \"insurance_price\"] = train_df.insurance_price.mean()\n",
    "    return train_df, test_df\n",
    "    \n",
    "def fix_registration_year(train_df, test_df):\n",
    "    train_df.loc[train_df.registration_year < 100, \"is_fixed_reg_year\"] = 1.0\n",
    "    train_df.registration_year = train_df.registration_year.apply(lambda y : 2000 + y if y < 21 else y)\n",
    "    train_df.registration_year = train_df.registration_year.apply(lambda y : 1900 + y if y < 100 else y)\n",
    "    \n",
    "    test_df.loc[test_df.registration_year < 100, \"is_fixed_reg_year\"] = 1.0\n",
    "    test_df.registration_year = test_df.registration_year.apply(lambda y : 2000 + y if y < 21 else y)\n",
    "    test_df.registration_year = test_df.registration_year.apply(lambda y : 1900 + y if y < 100 else y)\n",
    "    return train_df, test_df\n",
    "\n",
    "def cat_encode(train_df, test_df):\n",
    "    for cat_feature in cat_features:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(pd.concat([train_df, test_df])[cat_feature])\n",
    "        train_df[cat_feature] = le.transform(train_df[cat_feature])\n",
    "        test_df[cat_feature] = le.transform(test_df[cat_feature])\n",
    "        \n",
    "    return train_df, test_df\n",
    "\n",
    "def indicate_missing(train_df, test_df):\n",
    "    for missing_feature in cont_missing_features+cat_missing_features:\n",
    "        imp = MissingIndicator(missing_values=np.nan)\n",
    "        imp.fit(pd.concat([train_df, test_df])[[missing_feature]])\n",
    "        train_df[\"is_missing_\" + missing_feature] = imp.transform(train_df[[missing_feature]])\n",
    "        test_df[\"is_missing_\" + missing_feature] = imp.transform(test_df[[missing_feature]])\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks Functions\n",
    "* Function to plot DNN logs (MAPE)\n",
    "* Function to break DNN training when reach some accuracy (MAPE) value:\n",
    "* Identity block for ResNet\n",
    "* Dense block for ResNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hs_plot(history):\n",
    "    ''' history plot '''\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    mape = history.history['mape']\n",
    "    val_mape = history.history['val_mape']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, color='red', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, color='deeppink', label='Validation loss')\n",
    "    plt.plot(epochs, mape, color='lime', label='mape')\n",
    "    plt.plot(epochs, val_mape, color='green', label='Validation mape')\n",
    "    plt.title('Training and validation loss & Metrics(accuracy)')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss / acc')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.savefig('hist.png')\n",
    "    plt.show()\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('loss')<1):\n",
    "            print(\"\\nMAPE reached 1.5 so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "    \n",
    "def identity_block(input_tensor,units):\n",
    "    '''he identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "    input_tensor: input tensor\n",
    "    units:output shape\n",
    "\t# Returns\n",
    "\tOutput tensor for the block.\n",
    "\t'''\n",
    "    x = Dense(units, activation='relu')(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.75)(x)\n",
    "    x = Dense(units, activation='relu', kernel_regularizer=regularizers.l2(0.25))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.75)(x)\n",
    "    x = Dense(units, activation='relu', kernel_regularizer=regularizers.l2(0.25))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.75)(x)\n",
    "    x = add([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def dens_block(input_tensor,units):\n",
    "    ''' A block that has a dense layer at shortcut.\n",
    "    # Arguments\n",
    "    input_tensor: input tensor\n",
    "    unit: output tensor shape\n",
    "    # Returns\n",
    "    Output tensor for the block.\n",
    "    '''\n",
    "    x = Dense(units, activation='relu')(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.75)(x)\n",
    "    x = Dense(units, activation='relu', kernel_regularizer=regularizers.l2(0.25))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.75)(x)\n",
    "    x = Dense(units, activation='relu', kernel_regularizer=regularizers.l2(0.25))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.75)(x)\n",
    "    shortcut = Dense(units)(input_tensor)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "    x = add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(\n",
    "    DNN,\n",
    "    model,\n",
    "    train_df,\n",
    "    kfold,\n",
    "    metric,\n",
    "    preproc_funcs,\n",
    "    target=\"price\",\n",
    "    test_df=None,\n",
    "    log_target=False,\n",
    "    *args,\n",
    "    **kwargs\n",
    "):\n",
    "    val_scores = []\n",
    "    test_preds = []\n",
    "    train_df = train_df.drop_duplicates()\n",
    "    \n",
    "    if isinstance(kfold, GroupKFold):\n",
    "        splits = kfold.split(train_df, groups=kwargs[\"groups\"])\n",
    "    elif isinstance(kfold, StratifiedKFold):\n",
    "        target_values = train_df[[target]]\n",
    "        est = KBinsDiscretizer(n_bins=50, encode='ordinal', strategy='quantile')\n",
    "        stratify_on = est.fit_transform(target_values).T[0]\n",
    "        splits = kfold.split(train_df, stratify_on)\n",
    "    else:\n",
    "        splits = kfold.split(train_df)\n",
    "\n",
    "    for idx, (tr_idx, val_idx) in enumerate(splits):\n",
    "        tr_df = train_df.iloc[tr_idx]\n",
    "        val_df = train_df.iloc[val_idx]\n",
    "        \n",
    "        if test_df is not None:\n",
    "            tr_df, zip_df = preprocessing(tr_df, zip_dataframes(val_df, test_df), preproc_funcs)\n",
    "            val_df, ts_df = unzip_dataframes(zip_df)\n",
    "        else:\n",
    "            tr_df, val_df = preprocessing(tr_df, val_df, preproc_funcs)\n",
    "        \n",
    "        x_tr = tr_df.drop(columns=target).values\n",
    "        y_tr = tr_df[target].values\n",
    "        x_val = val_df.drop(columns=target).values\n",
    "        y_val = val_df[target].values\n",
    "        \n",
    "        if log_target:\n",
    "            y_tr = np.log(y_tr)\n",
    "            y_val = np.log(y_val)\n",
    "        \n",
    "        x_tr = np.asarray(x_tr).astype(np.float32)\n",
    "        y_tr = np.asarray(y_tr).astype(np.float32)\n",
    "        x_val = np.asarray(x_val).astype(np.float32)\n",
    "        \n",
    "        if DNN: model.fit(x_tr, y_tr, epochs=5,\n",
    "                           verbose=1,\n",
    "                           batch_size=32,\n",
    "                           callbacks=[callbacks],\n",
    "                           validation_split=0.05)\n",
    "        else:\n",
    "            model.fit(x_tr, y_tr)\n",
    "        preds = model.predict(x_val)\n",
    "        \n",
    "        preds = np.exp(preds) if log_target else preds\n",
    "        y_val = np.exp(y_val) if log_target else y_val\n",
    "        \n",
    "        fold_score = metric(y_val, preds)\n",
    "        val_scores.append(fold_score)\n",
    "        \n",
    "        print(f\"fold {idx+1} score: {fold_score}\")\n",
    "\n",
    "        if test_df is not None:\n",
    "            x_ts = ts_df.drop(columns=target).values\n",
    "            x_ts = np.asarray(x_ts).astype(np.float32)\n",
    "            test_fold_preds = model.predict(x_ts)\n",
    "            test_fold_preds = np.exp(test_fold_preds) if log_target else test_fold_preds\n",
    "            test_preds.append(test_fold_preds)\n",
    "            \n",
    "    print(f\"mean score: {np.mean(val_scores)}\")\n",
    "    print(f\"score variance: {np.var(val_scores)}\")\n",
    "\n",
    "    if test_df is not None:\n",
    "        return val_scores, test_preds\n",
    "    \n",
    "    return val_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LGBM Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 score: 25.342916622216453\n",
      "fold 2 score: 25.96884289647295\n",
      "fold 3 score: 25.082276540154382\n",
      "fold 4 score: 25.092093123317742\n",
      "fold 5 score: 25.46615069228981\n",
      "mean score: 25.390455974890266\n",
      "score variance: 0.10530321330907041\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LGBMRegressor(\n",
    "    random_state=42,\n",
    "    objective='mape',\n",
    "    num_leaves=100,\n",
    "    max_depth=-1,\n",
    "    learning_rate=0.03,\n",
    "    num_iterations=100,\n",
    "    subsample=0.5)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "preproc_funcs = [indicate_missing, impute_nan_with_zero, drop_columns, cat_encode]\n",
    "\n",
    "val_scores, test_preds = cross_validate(False,\n",
    "    model, \n",
    "    train_df,\n",
    "    kfold,\n",
    "    mape,\n",
    "    preproc_funcs,\n",
    "    test_df=test_df,\n",
    "    log_target=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run XGBoost Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 score: 22.882582724327257\n",
      "fold 2 score: 23.19899623333632\n",
      "fold 3 score: 22.51334350172197\n",
      "fold 4 score: 22.675473423983387\n",
      "fold 5 score: 23.065459344645276\n",
      "mean score: 22.86717104560284\n",
      "score variance: 0.06232112670718617\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = XGBRegressor(colsample_bytree=0.6,\n",
    "                         gamma=0.6,\n",
    "                         learning_rate=0.1,\n",
    "                         max_depth=20,\n",
    "                         min_child_weight=6,\n",
    "                         n_estimators=200,\n",
    "                         nthread=-1,\n",
    "                         reg_alpha=0.8,\n",
    "                         subsample=1,\n",
    "                         random_state=42,\n",
    "                         objective='reg:squarederror')\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "preproc_funcs = [indicate_missing, impute_nan_with_zero, drop_columns, cat_encode]\n",
    "\n",
    "val_scores, test_preds = cross_validate(False,\n",
    "    model, \n",
    "    train_df,\n",
    "    kfold,\n",
    "    mape,\n",
    "    preproc_funcs,\n",
    "    test_df=test_df,\n",
    "    log_target=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network (ResNet50) assembling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = myCallback()\n",
    "width = 16\n",
    "net_input = Input(shape=(25,))\n",
    "x = dens_block(net_input, width)\n",
    "x = identity_block(x, width)\n",
    "x = identity_block(x, width)\n",
    "x = dens_block(x, width)\n",
    "x = identity_block(x, width)\n",
    "x = identity_block(x, width)\n",
    "x = dens_block(x, width)\n",
    "x = identity_block(x, width)\n",
    "x = identity_block(x, width)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(1, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ResNet50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1188/1188 [==============================] - 10s 8ms/step - loss: 93.7760 - val_loss: 62.8120\n",
      "Epoch 2/5\n",
      "1188/1188 [==============================] - 9s 8ms/step - loss: 60.2859 - val_loss: 54.7944\n",
      "Epoch 3/5\n",
      "1188/1188 [==============================] - 9s 8ms/step - loss: 50.7406 - val_loss: 46.3160\n",
      "Epoch 4/5\n",
      "1188/1188 [==============================] - 9s 8ms/step - loss: 42.2564 - val_loss: 38.0858\n",
      "Epoch 5/5\n",
      "1188/1188 [==============================] - 9s 8ms/step - loss: 34.4254 - val_loss: 30.7469\n",
      "fold 1 score: 97.96941124258271\n",
      "Epoch 1/5\n",
      "1188/1188 [==============================] - 10s 8ms/step - loss: 27.5973 - val_loss: 24.6078\n",
      "Epoch 2/5\n",
      "1188/1188 [==============================] - 10s 8ms/step - loss: 22.0227 - val_loss: 19.8546\n",
      "Epoch 3/5\n",
      "1188/1188 [==============================] - 10s 8ms/step - loss: 18.0714 - val_loss: 16.7341\n",
      "Epoch 4/5\n",
      "1188/1188 [==============================] - 9s 8ms/step - loss: 15.6869 - val_loss: 15.0215\n",
      "Epoch 5/5\n",
      "1188/1188 [==============================] - 10s 8ms/step - loss: 14.4778 - val_loss: 14.0629\n",
      "fold 2 score: 94.3550771178399\n",
      "Epoch 1/5\n",
      "1188/1188 [==============================] - 9s 8ms/step - loss: 13.6735 - val_loss: 13.4539\n",
      "Epoch 2/5\n",
      "1188/1188 [==============================] - 9s 8ms/step - loss: 13.0902 - val_loss: 12.9556\n",
      "Epoch 3/5\n",
      "1188/1188 [==============================] - 9s 8ms/step - loss: 12.7862 - val_loss: 12.7515\n",
      "Epoch 4/5\n",
      "1188/1188 [==============================] - 9s 8ms/step - loss: 12.5421 - val_loss: 12.6371\n",
      "Epoch 5/5\n",
      "1188/1188 [==============================] - 9s 8ms/step - loss: 12.5272 - val_loss: 12.6596\n",
      "fold 3 score: 95.79938993209376\n",
      "Epoch 1/5\n",
      "1188/1188 [==============================] - 10s 9ms/step - loss: 12.5268 - val_loss: 12.4112\n",
      "Epoch 2/5\n",
      "1188/1188 [==============================] - 10s 8ms/step - loss: 12.2524 - val_loss: 12.0624\n",
      "Epoch 3/5\n",
      "1188/1188 [==============================] - 10s 8ms/step - loss: 12.2075 - val_loss: 12.1363\n",
      "Epoch 4/5\n",
      "1188/1188 [==============================] - 9s 8ms/step - loss: 12.2237 - val_loss: 12.0559\n",
      "Epoch 5/5\n",
      "1188/1188 [==============================] - 10s 9ms/step - loss: 12.0446 - val_loss: 11.9134\n",
      "fold 4 score: 103.05141043999602\n",
      "Epoch 1/5\n",
      "1188/1188 [==============================] - 10s 9ms/step - loss: 11.9566 - val_loss: 12.0174\n",
      "Epoch 2/5\n",
      "1188/1188 [==============================] - 10s 8ms/step - loss: 11.9255 - val_loss: 12.0246\n",
      "Epoch 3/5\n",
      "1188/1188 [==============================] - 10s 8ms/step - loss: 11.9295 - val_loss: 12.0474\n",
      "Epoch 4/5\n",
      "1188/1188 [==============================] - 10s 8ms/step - loss: 11.8654 - val_loss: 11.8918\n",
      "Epoch 5/5\n",
      "1188/1188 [==============================] - 10s 8ms/step - loss: 11.7709 - val_loss: 11.8572\n",
      "fold 5 score: 101.17995847771704\n",
      "mean score: 98.47104944204588\n",
      "score variance: 10.529705725798296\n",
      "Wall time: 4min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Model(inputs=net_input, outputs=x)\n",
    "model.compile(loss='mean_absolute_percentage_error',\n",
    "              optimizer='adam')\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "preproc_funcs = [indicate_missing, impute_nan_with_zero, drop_columns, cat_encode]\n",
    "\n",
    "\n",
    "val_scores, test_preds = cross_validate(True,\n",
    "    model, \n",
    "    train_df,\n",
    "    kfold,\n",
    "    mape,\n",
    "    preproc_funcs,\n",
    "    test_df=test_df,\n",
    "    log_target=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Running the classic model through 5 cross validation iteration we received more or less stable results where XGBoost showed a bit better results. Meanwhile **DNN** results are not so good. When **XGBoost** gives around 23 MAPE (Mean Absolut percentage error), **DNN** shows only around 100. \n",
    "\n",
    "The problem is in overfitting. Yes, taking into account a lot of problems with the date, as it usually happens in a real world, overfiting here is a real problem. Even regularization L2 with lambda up to 0.5 or dropout of layers up to 0.5 didn’t give any significant results. The best stabile results where reached with dropout 0.75 and L2 regularization lambda 0.75. \n",
    "\n",
    "So, the conclusion is that **DNN** is to complicate for such simple tasks like Regression and classic models like **XGBoost** or **LightGBM** demonstrate really great results.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
