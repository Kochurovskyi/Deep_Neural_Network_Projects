# Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization
[https://www.coursera.org/learn/deep-neural-network?specialization=deep-learning]

## Objectives:
  - Recall that different types of initializations lead to different results
  - Recognize the importance of initialization in complex neural networks.
  - Recognize the difference between train/dev/test sets
  - Diagnose the bias and variance issues in your model
  - Learn when and how to use regularization methods such as dropout or L2 regularization.
  - Understand experimental issues in deep learning such as Vanishing or Exploding gradients and learn how to deal with them
  - Use gradient checking to verify the correctness of your backpropagation implementation
  - Remember different optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam
  - Use random minibatches to accelerate the convergence and improve the optimization
  - Know the benefits of learning rate decay and apply it to your optimization
  - Master the process of hyperparameter tuning
  
## Week 1:
 - Initialization([Practical exercise](https://github.com/Kochurovskyi/Deep_Neural_Network_Projects/blob/main/Courses%20(COURSERA)/1.%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Logistic%20Regression%20as%20a%20Neural%20Network/Logistic_Regression_with_a_Neural_Network_mindset_v6a.ipynb))
 - Regularization
 - Gradient Checking
 
 ## Week 2:
-   Optimization methods
 
 ## Week 3:
-   Tensorflow Tutorial
![Cert.](https://github.com/Kochurovskyi/Deep_Neural_Network_Projects/blob/main/Courses%20(COURSERA)/2.%20Improving%20Deep%20Neural%20Networks/cert.png)

[Video](https://www.youtube.com/watch?v=1waHlpKiNyY&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc)
